<!DOCTYPE html>
<html><head></head><body><div class="main-panel wiki-content">
<div class="description">
<div class="header-wrapper" style="max-width: initial">
<!-- breadcrumbs could be implemented and inserted here -->


</div>
<div class="header-wrapper">
<div class="title-wrapper">
<div class="title">
<h1 style="display: flex; align-items: center;">
                                        Explainability - Technical Essentials
                                    </h1>
</div>

</div>
</div>
<div class="content-wrapper">
<style>@media (prefers-color-scheme: dark) { }</style>
<div class="toc-macro client-side-toc-macro conf-macro output-block" data-cssliststyle="none" data-hasbody="false" data-headerelements="H1,H2,H3" data-layout="default" data-local-id="ba631c81-69e5-4fb9-b033-ce1bb3a8b963" data-macro-id="aa53370e-4a80-45d1-aba1-abe6c1ef8cf9" data-macro-name="toc" data-numberedoutline="false" data-structure="list">
<style>[data-colorid=n00gllhzy4]{color:#0747a6} html[data-color-mode=dark] [data-colorid=n00gllhzy4]{color:#5999f8}[data-colorid=opmz92rpl5]{color:#0747a6} html[data-color-mode=dark] [data-colorid=opmz92rpl5]{color:#5999f8}[data-colorid=holb844ymi]{color:#4c9aff} html[data-color-mode=dark] [data-colorid=holb844ymi]{color:#004eb3}[data-colorid=vvi96v9cnn]{color:#0747a6} html[data-color-mode=dark] [data-colorid=vvi96v9cnn]{color:#5999f8}[data-colorid=amzeuiq6lp]{color:#0747a6} html[data-color-mode=dark] [data-colorid=amzeuiq6lp]{color:#5999f8}[data-colorid=ihs7386lx2]{color:#4c9aff} html[data-color-mode=dark] [data-colorid=ihs7386lx2]{color:#004eb3}[data-colorid=ttik3c6bph]{color:#0747a6} html[data-color-mode=dark] [data-colorid=ttik3c6bph]{color:#5999f8}[data-colorid=semv7ehihr]{color:#4c9aff} html[data-color-mode=dark] [data-colorid=semv7ehihr]{color:#004eb3}[data-colorid=dzocenp0ik]{color:#0747a6} html[data-color-mode=dark] [data-colorid=dzocenp0ik]{color:#5999f8}[data-colorid=wixhf667pv]{color:#0747a6} html[data-color-mode=dark] [data-colorid=wixhf667pv]{color:#5999f8}[data-colorid=gt0ut2x7mo]{color:#4c9aff} html[data-color-mode=dark] [data-colorid=gt0ut2x7mo]{color:#004eb3}[data-colorid=y6hgeprk2p]{color:#0747a6} html[data-color-mode=dark] [data-colorid=y6hgeprk2p]{color:#5999f8}[data-colorid=p9n4yu5pe9]{color:#4c9aff} html[data-color-mode=dark] [data-colorid=p9n4yu5pe9]{color:#004eb3}[data-colorid=y6ysjklrt4]{color:#0747a6} html[data-color-mode=dark] [data-colorid=y6ysjklrt4]{color:#5999f8}[data-colorid=osezrvx9jz]{color:#4c9aff} html[data-color-mode=dark] [data-colorid=osezrvx9jz]{color:#004eb3}[data-colorid=nq16bo795f]{color:#4c9aff} html[data-color-mode=dark] [data-colorid=nq16bo795f]{color:#004eb3}[data-colorid=d01m6t726y]{color:#0747a6} html[data-color-mode=dark] [data-colorid=d01m6t726y]{color:#5999f8}[data-colorid=og38apk4dg]{color:#0747a6} html[data-color-mode=dark] [data-colorid=og38apk4dg]{color:#5999f8}[data-colorid=bzfr9tfdva]{color:#4c9aff} html[data-color-mode=dark] [data-colorid=bzfr9tfdva]{color:#004eb3}[data-colorid=waje4ptpit]{color:#0747a6} html[data-color-mode=dark] [data-colorid=waje4ptpit]{color:#5999f8}[data-colorid=eskfdh0j9o]{color:#0747a6} html[data-color-mode=dark] [data-colorid=eskfdh0j9o]{color:#5999f8}</style>
<ul>
<li><a class="not-blank" href="#Overview"><span data-colorid="dzocenp0ik">Overview</span></a></li>
<li><a class="not-blank" href="#KeyDimensionsofExplainability"><span data-colorid="eskfdh0j9o">Key Dimensions of Explainability</span></a></li>
<li><a class="not-blank" href="#1AttentionVisualization"><span data-colorid="ttik3c6bph">1. Attention Visualization</span></a></li>
<li><a class="not-blank" href="#2ModelInterpretability"><span data-colorid="y6ysjklrt4">2. Model Interpretability</span></a>
<ul>
<li><a class="not-blank" href="#GlobalInterpretability"><strong><span data-colorid="osezrvx9jz">Global Interpretability</span></strong></a></li>
<li><a class="not-blank" href="#LocalInterpretability"><strong><span data-colorid="ihs7386lx2">Local Interpretability</span></strong></a></li>
<li><a class="not-blank" href="#ListofModelInterpretabilityTechniques"><span data-colorid="nq16bo795f">List of Model Interpretability Techniques</span></a></li>
</ul></li>
<li><a class="not-blank" href="#3OutputDiversification"><span data-colorid="amzeuiq6lp">3. Output Diversification</span></a>
<ul>
<li><a class="not-blank" href="#EnsembleMethods"><strong><span data-colorid="bzfr9tfdva">Ensemble Methods</span></strong></a></li>
<li><a class="not-blank" href="#CounterfactualExplanations"><strong><span data-colorid="holb844ymi">Counterfactual Explanations </span></strong></a></li>
<li><a class="not-blank" href="#PerturbationTechniques"><strong><span data-colorid="semv7ehihr">Perturbation Techniques</span></strong></a></li>
<li><a class="not-blank" href="#FeatureAttributionMethods"><strong><span data-colorid="p9n4yu5pe9">Feature Attribution Methods</span> </strong></a></li>
<li><a class="not-blank" href="#ExplanationClusteringTechniques"><strong><span data-colorid="gt0ut2x7mo">Explanation Clustering Techniques</span></strong></a></li>
</ul></li>
<li><a class="not-blank" href="#4ErrorAnalysis"><span data-colorid="n00gllhzy4">4. Error Analysis</span></a></li>
<li><a class="not-blank" href="#5TrainingDataTransparency"><span data-colorid="vvi96v9cnn">5. Training Data Transparency</span></a></li>
<li><a class="not-blank" href="#ResearchandFindings"><span data-colorid="opmz92rpl5">Research and Findings</span></a></li>
<li><a class="not-blank" href="#UnderstandingExplainabilityinAIEUAIActNISTISO42001"><span data-colorid="d01m6t726y">Understanding Explainability in AI: EU AI Act, NIST, ISO42001</span></a>
<ul>
<li><a class="not-blank" href="#NISTNationalInstituteofStandardsandTechnologyAIRMF"><span data-colorid="og38apk4dg">NIST (National Institute of Standards and Technology) - AI RMF</span></a></li>
<li><a class="not-blank" href="#EuropeanUnionArtificialIntelligenceActEUAIAct"><span data-colorid="waje4ptpit">European Union Artificial Intelligence Act (EU AI Act)</span></a></li>
<li><a class="not-blank" href="#ISO42001InternationalStandardforAIGovernance"><span data-colorid="y6hgeprk2p">ISO 42001 - International Standard for AI Governance</span></a></li>
</ul></li>
<li><a class="not-blank" href="#Glossaryoftermsused"><span data-colorid="wixhf667pv">Glossary of terms used</span></a></li>
</ul>
</div>
<h2 id="Overview"><span data-colorid="dzocenp0ik">Overview</span><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<p>Transparency and accountability are fundamental for responsible AI model usage. To achieve this, all AI models, including traditional machine learning and generative AI, require explanations across multiple dimensions. While researchers may employ diverse approaches to explainability, our focus is on five key areas: attention visualization, model interpretability, output diversification, error analysis, and training data transparency. Infosys Responsible AI toolkit is continually enriched by incorporating techniques and methods from ongoing research within these areas.</p>
<h2 id="KeyDimensionsofExplainability"><span data-colorid="eskfdh0j9o">Key Dimensions of Explainability</span><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="1121" style="max-width: 760px;" width="760"><img alt="image-20240918-130822.png" class="confluence-embedded-image image-center cursor-pointer" data-height="551" data-linked-resource-container-id="982974478" data-linked-resource-container-version="16" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="image-20240918-130822.png" data-linked-resource-id="1009287350" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="2e9b804c-5035-4d1b-9e83-cb6396ed6379" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/931823751/982974478/media/cTYzTHhIdWt3V1gtN09lVnR2VkpvZE1XNlNYZm1lMVlOUzBvZXU3aEtRcz0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVEkzTVRJME1UYzFOUT09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5PVGd5T1RjME5EYzQuYVcxaFoyVXRNakF5TkRBNU1UZ3RNVE13T0RJeUxuQnVadz09LmRtVnljMmx2YmoweEptMXZaR2xtYVdOaGRHbHZia1JoZEdVOU1UY3lOalkyTkRrd05UQXhNU1pqWVdOb1pWWmxjbk5wYjI0OU1TWmhjR2s5ZGpJbWQybGtkR2c5TnpZd0ptaGxhV2RvZEQwek56TT0=/image-20240918-130822.png" data-unresolved-comment-count="0" data-width="1121" loading="lazy" name="image-attachment" src="/docs/images\image-20240918-130822.png" style="width: 760px;" width="760"/></span>
<h2 id="1AttentionVisualization"><span data-colorid="ttik3c6bph">1. Attention Visualization</span> <a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<div class="expand-container conf-macro output-block" data-hasbody="true" data-macro-id="dbc4af16-2406-48a2-b8da-2d2fdc1ff0f7" data-macro-name="expand" id="expander-193486465">
<div class="expand-control" id="expander-control-193486465" onclick="expandContent('expander-content-193486465', 'expander-control-193486465')">
<span class="expand-control-text">List of attention visualization techniques</span>
</div>
<div class="expand-content expand-hidden" id="expander-content-193486465">
<p>A technique used in machine learning to understand how a model focuses on different parts of an input sequence when making a prediction. It is particularly useful for models that use attention mechanisms, such as transformers, which are widely used in natural language processing (NLP) and computer vision tasks.</p>
<div class="table-wrap dt1065162984" style="max-width: 760px; width: 100%; margin: 10px auto 0px;">
<div style="overflow-x: auto;"><table border="1" class="confluenceTable" data-layout="default" data-local-id="ad50a10e-196b-408c-b376-38bc94af4363" data-table-width="760" style="border-collapse: collapse; width: 100%;">
<tbody>
<tr>
<th class="confluenceTh" style="border: 1px solid #ddd; padding: 8px; background-color: #79bcf3; text-align: left;"><p><strong>Technique</strong></p></th>
<th class="confluenceTh" style="border: 1px solid #ddd; padding: 8px; background-color: #79bcf3; text-align: left;"><p><strong>Purpose</strong></p></th>
<th class="confluenceTh" style="border: 1px solid #ddd; padding: 8px; background-color: #79bcf3; text-align: left;"><p><strong>Sample Output</strong></p></th>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Token Importance Charts</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>To understand the relative importance of individual tokens (e.g., words, sub words)</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="199" style="max-width: 760px;" width="199"><img alt="image-20240918-131133.png" class="confluence-embedded-image image-center cursor-pointer" data-height="106" data-linked-resource-container-id="982974478" data-linked-resource-container-version="16" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="image-20240918-131133.png" data-linked-resource-id="1008304592" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="a9486c57-766b-4d56-8c64-3946df176f23" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/931823751/982974478/media/RGJOdFNhVjB2MjRwcW1IekttY1RzdkRabG5fbDU3bjNFcUptemdVbTFnbz0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVEkzTVRJME1UYzFOZz09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5PVGd5T1RjME5EYzQuYVcxaFoyVXRNakF5TkRBNU1UZ3RNVE14TVRNekxuQnVadz09LmRtVnljMmx2YmoweEptMXZaR2xtYVdOaGRHbHZia1JoZEdVOU1UY3lOalkyTlRBNU9EVXhPQ1pqWVdOb1pWWmxjbk5wYjI0OU1TWmhjR2s5ZGpJbWQybGtkR2c5TVRrNUptaGxhV2RvZEQweE1EVT0=/image-20240918-131133.png" data-unresolved-comment-count="0" data-width="199" loading="lazy" name="image-attachment" src="/docs/images\image-20240918-131133.png" style="border: 2px solid rgb(117, 129, 149); width: 199px;" width="199"/></span></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Heat Map</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>To highlight the most important regions or features of an input that contribute to a model's prediction.</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="152" style="max-width: 760px;" width="186"><img alt="image-20240918-131206.png" class="confluence-embedded-image image-center cursor-pointer" data-height="123" data-linked-resource-container-id="982974478" data-linked-resource-container-version="16" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="image-20240918-131206.png" data-linked-resource-id="1009680453" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="e69b78a9-5112-4ca4-83f8-d673d7f4fcf6" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/931823751/982974478/media/aTNzSDBPU3BiT0M5eDIxQjlsb3BXaHpDcldGb0hVTUJRbVJKRnFxQ0gyaz0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVEkzTVRJME1UYzFOdz09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5PVGd5T1RjME5EYzQuYVcxaFoyVXRNakF5TkRBNU1UZ3RNVE14TWpBMkxuQnVadz09LmRtVnljMmx2YmoweEptMXZaR2xtYVdOaGRHbHZia1JoZEdVOU1UY3lOalkyTlRFek1qSTROU1pqWVdOb1pWWmxjbk5wYjI0OU1TWmhjR2s5ZGpJbWQybGtkR2c5TVRnMkptaGxhV2RvZEQweE5UQT0=/image-20240918-131206.png" data-unresolved-comment-count="0" data-width="152" loading="lazy" name="image-attachment" src="/docs/images\image-20240918-131206.png" style="width: 186px;" width="186"/></span></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Saliency Map</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>To identify the most important regions of an image for a specific task (e.g., classification, object detection).</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="134" style="max-width: 760px;" width="182"><img alt="image-20240918-131312.png" class="confluence-embedded-image image-center cursor-pointer" data-height="107" data-linked-resource-container-id="982974478" data-linked-resource-container-version="16" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="image-20240918-131312.png" data-linked-resource-id="1009221793" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="4fb7a954-1fc0-4432-abb5-3616eadf3c92" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/931823751/982974478/media/WlZTZ3RTZXNZdW5FS2JVY0d6czJlQUtqMmtLRzBBZGRLdTBTeDEwZE5MYz0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVEkzTVRJME1UYzFPQT09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5PVGd5T1RjME5EYzQuYVcxaFoyVXRNakF5TkRBNU1UZ3RNVE14TXpFeUxuQnVadz09LmRtVnljMmx2YmoweEptMXZaR2xtYVdOaGRHbHZia1JoZEdVOU1UY3lOalkyTlRFNU5Ea3lOU1pqWVdOb1pWWmxjbk5wYjI0OU1TWmhjR2s5ZGpJbWQybGtkR2c5TVRneUptaGxhV2RvZEQweE5EUT0=/image-20240918-131312.png" data-unresolved-comment-count="0" data-width="134" loading="lazy" name="image-attachment" src="/docs/images\image-20240918-131312.png" style="width: 182px;" width="182"/></span></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Super pixels</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>To group pixels in an image into perceptually meaningful regions (e.g., objects, textures).</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="79" style="max-width: 760px;" width="125"><img alt="image-20240918-131341.png" class="confluence-embedded-image image-center cursor-pointer" data-height="120" data-linked-resource-container-id="982974478" data-linked-resource-container-version="16" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="image-20240918-131341.png" data-linked-resource-id="1008861386" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="7bde2b5b-e44e-4dd4-a39b-eb6248a610ab" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/931823751/982974478/media/YndpYU5VOWMxZEpsc3dWSFVEcm03dnFuelVCUWsxWEF0TmY2bjcyWTlmdz0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVEkzTVRJME1UYzFPQT09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5PVGd5T1RjME5EYzQuYVcxaFoyVXRNakF5TkRBNU1UZ3RNVE14TXpReExuQnVadz09LmRtVnljMmx2YmoweEptMXZaR2xtYVdOaGRHbHZia1JoZEdVOU1UY3lOalkyTlRJeU5ESTFNeVpqWVdOb1pWWmxjbk5wYjI0OU1TWmhjR2s5ZGpJbWQybGtkR2c5TVRJMUptaGxhV2RvZEQweE9EZz0=/image-20240918-131341.png" data-unresolved-comment-count="0" data-width="79" loading="lazy" name="image-attachment" src="/docs/images\image-20240918-131341.png" style="width: 125px;" width="125"/></span></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Circo's Plots</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>To represent relationships between entities in a circular format. They are particularly useful for visualizing complex data sets, such as genomic data, protein-protein interactions, and social networks.</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="161" style="max-width: 760px;" width="161"><img alt="image-20240918-131448.png" class="confluence-embedded-image image-center cursor-pointer" data-height="161" data-linked-resource-container-id="982974478" data-linked-resource-container-version="16" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="image-20240918-131448.png" data-linked-resource-id="1008926939" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="f802f8b1-871b-46d1-a7b4-c9d78f74fa2e" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/931823751/982974478/media/ckhvbDZGd21jamNnUk9sUWc1NzNPdVFfelVURnVMSHNNQy1qdmtoNFFIND0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVEkzTVRJME1UYzFPUT09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5PVGd5T1RjME5EYzQuYVcxaFoyVXRNakF5TkRBNU1UZ3RNVE14TkRRNExuQnVadz09LmRtVnljMmx2YmoweEptMXZaR2xtYVdOaGRHbHZia1JoZEdVOU1UY3lOalkyTlRJNU1EUTROQ1pqWVdOb1pWWmxjbk5wYjI0OU1TWmhjR2s5ZGpJbWQybGtkR2c5TVRZeEptaGxhV2RvZEQweE5qRT0=/image-20240918-131448.png" data-unresolved-comment-count="0" data-width="161" loading="lazy" name="image-attachment" src="/docs/images\image-20240918-131448.png" style="width: 161px" width="161"/></span></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Sankey Diagrams</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>To represent the flow of quantities through a system. They are particularly useful for understanding how input values are transformed into output values in a machine learning model.</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size" original-width="213" style="max-width: 760px;" width="213"><img alt="image-20240918-131632.png" class="confluence-embedded-image image-center cursor-pointer" data-height="102" data-linked-resource-container-id="982974478" data-linked-resource-container-version="16" data-linked-resource-content-type="image/png" data-linked-resource-default-alias="image-20240918-131632.png" data-linked-resource-id="1009058018" data-linked-resource-type="attachment" data-linked-resource-version="1" data-media-id="2449da4c-c52a-48fb-9124-7a559a6a53ed" data-media-type="file" data-thumbnail-url="/content/58f1bf1b-24d0-4a88-8e47-edf6a46686f0/207815356/228720962/931823751/982974478/media/TWxsMXJjdXZPbVhjVEZDUFJQRGxQN01tcngwUERZQkZxSlVLb2MwczFpZz0=.TlRobU1XSm1NV0l0TWpSa01DMDBZVGc0TFRobE5EY3RaV1JtTm1FME5qWTRObVl3Lk1UY3pPVEkzTVRJME1UYzJNQT09LmJ3PT0uZEdoMWJXSnVZV2xzY3c9PS5PVGd5T1RjME5EYzQuYVcxaFoyVXRNakF5TkRBNU1UZ3RNVE14TmpNeUxuQnVadz09LmRtVnljMmx2YmoweEptMXZaR2xtYVdOaGRHbHZia1JoZEdVOU1UY3lOalkyTlRNNU5UUTRNaVpqWVdOb1pWWmxjbk5wYjI0OU1TWmhjR2s5ZGpJbWQybGtkR2c5TWpFekptaGxhV2RvZEQweE1ERT0=/image-20240918-131632.png" data-unresolved-comment-count="0" data-width="213" loading="lazy" name="image-attachment" src="/docs/images\image-20240918-131632.png" style="width: 213px" width="213"/></span></td>
</tr>
</tbody>
</table></div>
<style>html .dt1065162984 table {min-width:532px;}</style>
</div>
</div>
</div>
<h2 id="2ModelInterpretability"><span data-colorid="y6ysjklrt4">2. Model Interpretability</span><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<div class="expand-container conf-macro output-block" data-hasbody="true" data-macro-id="ab6fdc54-77da-44f2-847a-7636e90f360b" data-macro-name="expand" id="expander-1743261496">
<div class="expand-control" id="expander-control-1743261496" onclick="expandContent('expander-content-1743261496', 'expander-control-1743261496')">
<span class="expand-control-text">Details of various model interpretability methods</span>
</div>
<div class="expand-content expand-hidden" id="expander-content-1743261496">
<p>Model interpretability aims at understanding how AI models arrive at its predictions. It involves breaking down the complex decision-making process of the model into human-understandable terms.</p>
<p>Feature Importance is a critical component of Model Interpretability, used to understand the relative importance of different features (variables) an AI model prediction. By quantifying the contribution of each feature, it helps to explain the model's decision-making process. </p>
<p>AI models require explanations of key features at both the overall model level and the individual instance level, termed global and local interpretability. Brief descriptions of terms related to model interpretability are outlined below</p>
<h3 id="GlobalInterpretability"><strong><span data-colorid="osezrvx9jz">Global Interpretability</span></strong> <a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>Understanding the overall behavior and decision-making process of a model across its entire input space. It provides insights into the general patterns and trends that the model has learned from the data. The following techniques help in providing Global explainability:</p>
<ul>
<li><p>Permutation importance</p></li>
<li><p>Mean decrease impurity</p></li>
<li><p>Decision trees</p></li>
<li><p>Rule-based induction</p></li>
<li><p>Surrogate models</p></li>
<li><p>Self-reasoning techniques</p></li>
</ul>
<h3 id="LocalInterpretability"><strong><span data-colorid="ihs7386lx2">Local Interpretability</span></strong><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>The ability to understand how a machine learning model arrives at its predictions for specific instances. Unlike global interpretability, which focuses on understanding the overall behavior of the model, local interpretability provides explanations tailored to individual predictions. The following techniques help in providing Local explainability:</p>
<ul>
<li><p>LIME</p></li>
<li><p>SHAP</p></li>
<li><p>Anchors</p></li>
<li><p>Counter factual explanations</p></li>
<li><p>Self-reasoning techniques</p></li>
</ul>
<h3 id="ListofModelInterpretabilityTechniques"><span data-colorid="nq16bo795f">List of Model Interpretability Techniques</span><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<h4 id="PermutationImportance"><strong>Permutation Importance</strong> <a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p>a technique used to assess the importance of a feature by measuring the change in model performance when the feature's values are shuffled randomly.</p>
<p>Purpose: Feature ranking, Model understanding, Feature selection, Bias detection</p>
<p>Applicability: All Machine learning models </p>
<h4 id="SHAPSHapleyAdditiveexPlanations"><strong>SHAP (SHapley Additive exPlanations)</strong><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p>a game-theoretic method for explaining the output of any machine learning model. It calculates the contribution of each feature to a prediction, providing a more comprehensive understanding of the model's decision-making process.</p>
<p>Purpose: Feature ranking, Model understanding, Bias detection, Debugging</p>
<p>Applicability: SHAP is a model-agnostic method and applied to wide range of machine learning models </p>
<p>(e.g., Decision trees, Random forests, Gradient boosting machines, Neural networks, Support vector machines)</p>
<h4 id="LIMELocalInterpretableModelAgnosticExplanations"><strong>LIME (Local Interpretable Model-Agnostic Explanations)</strong> <a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p>a technique used in machine learning to explain the predictions of complex models in a locally interpretable way. It works by approximating a complex model with a simpler, linear model around a specific prediction.</p>
<p>Purpose: Local interpretability, Model agnosticism, Feature importance, Bias detection, Debugging</p>
<p>Applicability: LIME is a model-agnostic method and applied to wide range of machine learning models </p>
<p>(e.g., Decision trees, Random forests, Gradient boosting machines, Neural networks, Support vector machines)</p>
<h4 id="AnchorTabular"><strong>Anchor Tabular</strong> <a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p>a technique used in explainability to identify a minimal set of conditions (anchors) that are sufficient to explain a prediction made by a machine learning model. These anchors are human-readable rules that capture the essence of the model's decision-making process for a specific instance.</p>
<p>Purpose: Local interpretability, Simplicity, Feature importance, Bias detection, Debugging</p>
<p>Applicability: Anchor Tabular is primarily designed for tabular data, but it can also be applied to other types of data with some modifications. It can be used to explain the predictions of various machine learning models (e.g., Decision trees, Random forests, Gradient boosting machines, Neural networks, Support vector machines) </p>
<h4 id="Counterfactualexplanations"><strong>Counterfactual explanations</strong> <a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p>a technique used in explainable AI to generate hypothetical scenarios that could have led to different predictions. By understanding how changes in input features would have affected the model's output, we can gain insights into the model's decision-making process.</p>
<p>Purpose: Local interpretability, Causality, Fairness, Debugging</p>
<p>Applicability: applied to wide range of machine learning models (e.g., Decision trees, Random forests, Gradient boosting machines, Neural networks, Linear models)</p>
<h4 id="MeanDecreaseImpurityMDI"><strong>Mean Decrease Impurity (MDI)</strong> <a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p>a metric used in machine learning to measure the importance of features in a decision tree model. It quantifies the average reduction in impurity (e.g., Gini impurity, entropy) that results from splitting on a particular feature.</p>
<p>Purpose: Feature importance, Feature selection, Model understanding, Bias detection</p>
<p>Applicability: MDI is primarily used to explain decision tree models, as it is a metric derived from the impurity measures used in decision trees. However, it can also be applied to ensemble models like Random Forests, where the average MDI across all trees can be used to assess feature importance.</p>
<h4 id="DecisionTrees"><strong>Decision Trees</strong> <a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p>a type of machine learning model that can be used for both classification and regression tasks. In the context of explainability, they are particularly valuable due to their inherent interpretability.</p>
<p>Purpose: Visual representation, Rule extraction, Feature importance, Local interpretability</p>
<p>Applicability: While decision trees themselves are highly interpretable, they can also be used to explain other types of machine learning models like Random Forests and Gradient Boosting Machines</p>
<h4 id="Rulebasedinduction"><strong>Rule-based induction</strong> <a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p>a technique used in machine learning to extract human-readable rules from a model. These rules can provide insights into the model's decision-making process and make it easier to understand and interpret.</p>
<p>Purpose: Interpretability, Understanding, Debugging, Knowledge extraction</p>
<p>Applicability: Rule-based induction can be applied to a variety of machine learning models, including:</p>
<ul>
<li><p>Decision Trees: Rules can be extracted directly from decision trees.</p></li>
<li><p>Rule-based Classifiers: Models that are explicitly designed to learn rules from data, such as RIPPER or CN2.</p></li>
<li><p>Neural Networks: Rules can be extracted from neural networks using techniques like rule extraction or symbolic regression.</p></li>
<li><p>Ensemble Models: Rules can be extracted from individual models in an ensemble and combined to provide a more comprehensive explanation.</p></li>
</ul>
<h4 id="Surrogatemodels"><strong>Surrogate models</strong> <a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p>simpler models that approximate the behavior of a more complex machine learning model. They are used in explainability to provide a more understandable representation of the complex model's decision-making process</p>
<p>Purpose: Interpretability, Approximation, Feature importance, Debugging</p>
<p>Applicability: Surrogate models can be used to explain a wide range of machine learning models, including:</p>
<ul>
<li><p>Deep Neural Networks: Complex neural networks can be approximated by simpler models like linear regression or decision trees.</p></li>
<li><p>Ensemble Models: Surrogate models can be used to explain the combined behavior of multiple models in an ensemble.</p></li>
<li><p>Black-box Models: Any machine learning model that is difficult to understand directly can be approximated by a surrogate model.</p></li>
</ul>
<p>Common Surrogate Models:</p>
<ul>
<li><p>Linear Regression: A simple linear model that can be used to approximate the relationship between input features and the target variable.</p></li>
<li><p>Decision Trees: Decision trees are often used as surrogate models due to their interpretability.</p></li>
<li><p>Rule-based Models: Models that are based on a set of rules can be used as surrogate models to explain the decision-making process.</p></li>
</ul>
<h4 id="Selfreasoningtechniques"><strong>Self-reasoning techniques</strong> <a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p>a set of techniques that enable LLMs to generate more comprehensive and coherent explanations for their outputs. These frameworks often involve breaking down complex problems into smaller, more manageable steps and guiding the LLM through a reasoning process. This technique relies heavily on prompt engineering, with thoughtfully crafted prompts leading to informative outcomes.</p>
<p>Few models built on self-reasoning framework are: </p>
<ul>
<li><p>Chain of Thought (CoT): Step-by-step reasoning technique prompts the LLM to break down a complex task into smaller, more manageable steps and explain its reasoning for each step. This allows users to follow the LLM's thought process and understand how it arrived at its conclusion.</p></li>
<li><p>Thread-of-Thought (ToT) : Generate a series of interconnected thoughts that form a coherent narrative. This helps to visualize the LLM's reasoning process and identify any inconsistencies or biases.</p></li>
<li><p>Graph-of-Thought (GoT) : represents the LLM's reasoning process as a graph, where nodes represent intermediate thoughts and edges represent the connections between them. This visual representation can be helpful for understanding the LLM's decision-making process.</p></li>
<li><p>Chain-of-Verification (CoV) : prompts the LLM to verify its responses against external knowledge sources. This helps to ensure the accuracy and reliability of the LLM's outputs.</p></li>
<li><p>ReRead Reasoning (RE2) : Unlike most thought eliciting prompting methods, such as Chain-of Thought (CoT), which aim to elicit the reasoning process in the output, RE2 shifts the focus to the input by processing questions twice, thereby enhancing the understanding process. Consequently, RE2 demonstrates strong generality and compatibility with most thought eliciting prompting methods.</p></li>
<li><p>Logic of Thoughts (LOT) : Logic-of-Thought (LoT) prompting which employs propositional logic to generate expanded logical information from input context, and utilizes the generated logical information as an additional augmentation to the input prompts, thereby enhancing the capability of logical reasoning. The LoT is orthogonal to existing prompting methods and can be seamlessly integrated with them.</p></li>
</ul>
<p>Purpose: Step-by-step breakdown, Coherent narratives, Accuracy and reliability, Transparency</p>
<p>Applicability: LLMs</p>
</div>
</div>
<h2 id="3OutputDiversification"><span data-colorid="amzeuiq6lp">3. Output Diversification</span><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<div class="expand-container conf-macro output-block" data-hasbody="true" data-macro-id="ba15581a-cade-4d5d-8dc5-23d257d49a93" data-macro-name="expand" id="expander-1797753284">
<div class="expand-control" id="expander-control-1797753284" onclick="expandContent('expander-content-1797753284', 'expander-control-1797753284')">
<span class="expand-control-text">Approaches for output diversification</span>
</div>
<div class="expand-content expand-hidden" id="expander-content-1797753284">
<p>Output diversification in explainability refers to the practice of generating multiple, diverse explanations for a given model prediction or decision. This is done to ensure that the explanations are not biased towards a particular perspective or interpretation.</p>
<h3 id="EnsembleMethods"><strong><span data-colorid="bzfr9tfdva">Ensemble Methods</span></strong><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>Combining multiple models or explanations can provide a more diverse set of insights. Few examples of ensembling are outlined below</p>
<h4 id="Bagging">Bagging<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p>Train multiple decision trees on different subsets of the data and combine their explanations.</p>
<h4 id="Boosting">Boosting<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p>Train a series of models, focusing on the instances that were misclassified by previous models, and combine their explanations.</p>
<h4 id="RandomForest">Random Forest<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p>Train multiple decision trees, each using a random subset of features and samples, and combine their explanations.</p>
<h4 id="Stacking">Stacking<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p>Train a logistic regression model to combine the explanations from multiple base models like decision trees, neural networks.</p>
<h4 id="Blending">Blending<a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h4>
<p>Train multiple models (ex. decision tree, random forest, neural network), use SHAP for feature importance, and average the results to obtain a combined explanation.</p>
<h3 id="CounterfactualExplanations"><strong><span data-colorid="holb844ymi">Counterfactual Explanations </span></strong><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>Generating alternative scenarios/hypothetical examples and evaluate the impact of specific input changes on the model’s outcome.</p>
<h3 id="PerturbationTechniques"><strong><span data-colorid="semv7ehihr">Perturbation Techniques</span></strong><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>Introducing small changes to the input data or model parameters can reveal different explanations. Examples of perturbation include:</p>
<ul>
<li><p>Input perturbation - Add random noise to the input variables to see how the explanation changes.</p></li>
<li><p>Model perturbation - Modify the weights of the model to see how the importance of different features changes.</p></li>
</ul>
<h3 id="FeatureAttributionMethods"><strong><span data-colorid="p9n4yu5pe9">Feature Attribution Methods</span> </strong><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>Using various feature attribution methods (e.g., SHAP, LIME) can offer different perspectives on the importance of input features.</p>
<h3 id="ExplanationClusteringTechniques"><strong><span data-colorid="gt0ut2x7mo">Explanation Clustering Techniques</span></strong><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<p>Group similar explanations together to identify patterns, trends, and relationships within a set of model explanations. Various clustering techniques include: </p>
<ul>
<li><p>K-means clustering</p></li>
<li><p>Hierarchical clustering</p></li>
<li><p>Density based clustering</p></li>
<li><p>Spectral clustering</p></li>
<li><p>Topic modeling</p></li>
</ul>
</div>
</div>
<h2 id="4ErrorAnalysis"><span data-colorid="n00gllhzy4">4. Error Analysis</span><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<div class="expand-container conf-macro output-block" data-hasbody="true" data-macro-id="e9406c58-4f56-4a49-8ab1-ad2d87a97de6" data-macro-name="expand" id="expander-602928425">
<div class="expand-control" id="expander-control-602928425" onclick="expandContent('expander-content-602928425', 'expander-control-602928425')">
<span class="expand-control-text">Key steps in error analysis</span>
</div>
<div class="expand-content expand-hidden" id="expander-content-602928425">
<p>Error analysis in explainability is the process of systematically examining the errors made by a machine learning model to understand why it made incorrect predictions. This involves analyzing the model's behavior, the input data, and the underlying reasoning behind its decisions. Key milestones in error analysis are outlined below.</p>
<ul>
<li><p><strong>Error Classification:</strong> Categorizing errors based on their nature. For example, false positives, false negatives, misclassifications.</p></li>
<li><p><strong>Root Cause Analysis:</strong> Investigate data quality issues, model complexity, or algorithmic biases.</p></li>
<li><p><strong>Bias Detection:</strong> Use fairness metrics or bias detection algorithms to uncover biases in the model's predictions</p></li>
<li><p><strong>Data Quality Assessment:</strong> Verify the quality of the input data, looking for missing values, outliers, or inconsistencies.</p></li>
<li><p><strong>Model Complexity Analysis:</strong> Determine if the model is too complex or too simple for the task.</p></li>
</ul>
</div>
</div>
<h2 id="5TrainingDataTransparency"><span data-colorid="vvi96v9cnn">5. Training Data Transparency</span><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<div class="expand-container conf-macro output-block" data-hasbody="true" data-macro-id="833b9d21-b89f-416f-9aef-ba48ec447520" data-macro-name="expand" id="expander-1888218140">
<div class="expand-control" id="expander-control-1888218140" onclick="expandContent('expander-content-1888218140', 'expander-control-1888218140')">
<span class="expand-control-text">Key aspects to consider for training data transparency</span>
</div>
<div class="expand-content expand-hidden" id="expander-content-1888218140">
<p>The practice of providing information about the data used to train a machine learning model is crucial for understanding the model's decision-making process. Hence clear documentation of following aspects of training data is required to ensure fairness and reliability of AI models</p>
<ul>
<li><p><strong>Data Quality: </strong>Verify the accuracy and reliability of the data and check for inconsistencies</p></li>
<li><p><strong>Data Privacy: </strong>Ensure compliance with relevant data privacy regulations and implement measures to safeguard the sensitive data and prevent unauthorized access</p></li>
<li><p><strong>Data characteristics:</strong> Outline the details about the data, such as its size, format, and distribution.</p></li>
<li><p><strong>Data preprocessing:</strong> Understand preprocessing steps applied to the data, such as normalization, scaling, or feature engineering and assess their impact on the model performance and interpretability</p></li>
<li><p><strong>Data biases:</strong> Use bias detection algorithms or statistical analysis to identify potential biases in the data and implement mitigation strategies</p></li>
<li><p><strong>Data provenance:</strong> Keep detailed records of data sources, collection methods, and preprocessing steps and provide access to these records to the stakeholders</p></li>
</ul>
</div>
</div>
<hr/>
<h2 id="ResearchandFindings"><span data-colorid="opmz92rpl5">Research and Findings</span><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<div class="expand-container conf-macro output-block" data-hasbody="true" data-macro-id="94a4e7c0-7d84-4cee-9754-4b73ddec7cbc" data-macro-name="expand" id="expander-1370281297">
<div class="expand-control" id="expander-control-1370281297" onclick="expandContent('expander-content-1370281297', 'expander-control-1370281297')">
<span class="expand-control-text">List of Research areas and findings related to Explainability tenet</span>
</div>
<div class="expand-content expand-hidden" id="expander-content-1370281297">
<div class="table-wrap dt1004759711 table-responsive" style="max-width: 760px; width: 100%; margin: 10px auto 0px;">
<div style="overflow-x: auto;"><table border="1" class="confluenceTable" data-layout="default" data-local-id="df5c3107-6009-459c-a508-0924158438c1" data-table-width="760" style="border-collapse: collapse; width: 100%;">
<tbody>
<tr>
<th class="confluenceTh" style="border: 1px solid #ddd; padding: 8px; background-color: #79bcf3; text-align: left;"><p><strong>Category</strong></p></th>
<th class="confluenceTh" style="border: 1px solid #ddd; padding: 8px; background-color: #79bcf3; text-align: left;"><p><strong>Research Topic</strong></p></th>
<th class="confluenceTh" style="border: 1px solid #ddd; padding: 8px; background-color: #79bcf3; text-align: left;"><p><strong>Reference(s)</strong></p></th>
<th class="confluenceTh" style="border: 1px solid #ddd; padding: 8px; background-color: #79bcf3; text-align: left;"><p><strong>Summary/ Key takeaways</strong></p></th>
<th class="confluenceTh" style="border: 1px solid #ddd; padding: 8px; background-color: #79bcf3; text-align: left;"><p><strong>Conclusion remarks</strong></p></th>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Attention Visualization</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Feature Importance Charts</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><a class="external-link" href="https://github.com/jessevig/bertviz" rel="nofollow" target="_blank">jessevig/bertviz: BertViz: Visualize Attention in NLP Models (BERT, GPT2, BART, etc.) (github.com)</a></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Considered to implement visualization of importance of Tokens. </p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Implemented attention visualization for unstructured Text. Further research is going on to implement attention visualization for Image/Video.</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Chain of Thought (CoT)</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Output Diversification / LLM response reasoning</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><a class="external-link" href="https://arxiv.org/abs/2201.11903" rel="nofollow" target="_blank">[2201.11903] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (arxiv.org)</a></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Considered to implement Chain of thought reasoning to understand LLM Response</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Implemented Chain of though reasoning in RAI toolkit. We use the same COT to benchmark various LLM’s capability of Reasoning.</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Thread of Thought (ToT)</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Output Diversification / LLM response reasoning</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><a class="external-link" href="https://arxiv.org/pdf/2311.08734" rel="nofollow" target="_blank">https://arxiv.org/pdf/2311.08734</a></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Considered to implement Thread of thought reasoning to provide explanation to understand LLM Response </p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Implemented Thread of though reasoning in RAI toolkit. We use the same to benchmark various LLM’s capability of Reasoning.</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Graph of Thought (GoT)</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Output Diversification / LLM response reasoning</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><a class="external-link" href="https://arxiv.org/abs/2308.09687" rel="nofollow" target="_blank">[2308.09687] Graph of Thoughts: Solving Elaborate Problems with Large Language Models (arxiv.org)</a></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Considered to implement Graph of thought reasoning to provide consistent explanation to understand LLM Response </p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Implemented Graph of though reasoning in RAI toolkit. We use the same to benchmark various LLM’s capability of Reasoning.</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>LIME</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Global/Local Explainability, Output Diversification</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><a class="external-link" data-card-appearance="inline" href="https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_tabular" rel="nofollow" target="_blank">https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_tabular</a> </p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Considered using LIME packages to get explanation for traditional AI</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>RAI Workbench is integrated with LIME packages to provide explanation for model prediction</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>SHAP</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Global/Local Explainability, Output Diversification</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><a class="external-link" href="https://shap.readthedocs.io/en/latest/" rel="nofollow" target="_blank">Welcome to the SHAP documentation — SHAP latest documentation</a></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Considered using SHAP packages to get explanation for traditional AI</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>RAI Workbench is integrated with SHAP packages to provide explanation for model prediction</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Internet Search</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Factual Check for LLM response explainability</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><a class="external-link" href="https://arxiv.org/pdf/2403.18802" rel="nofollow" target="_blank">https://arxiv.org/pdf/2403.18802</a></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Considered the idea of taking internet data to verify LLM response. From this research paper idea of verifying factual data takes high computation time So considered to implement factual verification by LLM itself.</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Implemented verification of LLM response with Internet Search</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Internet Search</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Factual Check for LLM Response</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><a class="external-link" data-card-appearance="inline" href="https://www.linkedin.com/pulse/review-perplexity-ai-revolutionizing-your-everyday-sigmund-0jptc/?trackingId=pzMALFLtnViokPFsfsHucw%3D%3D" rel="nofollow" target="_blank">https://www.linkedin.com/pulse/review-perplexity-ai-revolutionizing-your-everyday-sigmund-0jptc/?trackingId=pzMALFLtnViokPFsfsHucw==</a> </p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Perplexity AI api is a cheaper alternative compared to Google’s SerperAPI to retrieve live information from website to enable factuality check of LLM responses. </p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Implementing as a part of Internet Search validation of LLM Responses (WIP as on 9-Oct-2024)</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>ReRead Prompt</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Re Read the Prompt for Better Explainability</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><a class="external-link" href="https://arxiv.org/pdf/2309.06275" rel="nofollow" target="_blank">https://arxiv.org/pdf/2309.06275</a></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Considered the prompt Engineering technique to ask the LLM to reread the prompt. Various Benchmarking was already done to prove the effectiveness of this.</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Exploring to use as explainability enhancement feature. Research in progress (as on 9-Oct-24)</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Attention Visualization</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>SuperPixel</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><a class="external-link" data-card-appearance="inline" href="https://cran.r-project.org/web/packages/OpenImageR/vignettes/Image_segmentation_superpixels_clustering.html" rel="nofollow" target="_blank">https://cran.r-project.org/web/packages/OpenImageR/vignettes/Image_segmentation_superpixels_clustering.html</a> </p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Considered to implement attention visualization for Images to have fair understanding on How AI model understands Image.</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>WIP to implement SuperPixel. Super Pixel is a group of pixels which preserve important information of the image.</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Benchmarking</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Explainability Benchmarking</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><a class="external-link" href="http://vimptmast-07:5019/" rel="nofollow" target="_blank">http://vimptmast-07:5019/</a></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>There is no precise benchmarking tool available So implementing Infosys Responsible AI Explainability Benchmarking by using Infosys Responsible AI Features</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>WIP to Load and test the Explanation capability of Various LLM’s by using Infosys responsible AI Features</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Logic of Thought</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Output Diversification / LLM response reasoning</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><a class="external-link" href="https://arxiv.org/abs/2409.17539" rel="nofollow" target="_blank">[2409.17539] Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models</a></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Logic-of-Thought (LoT) prompting which employs propositional logic to generate expanded logical information from input context, and utilizes the generated logical information as an additional augmentation to the input prompts, thereby enhancing the capability of logical reasoning. </p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Considered as an additional Explanation method and added to RAI toolkit</p></td>
</tr>
<tr>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Evaluation</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>QUEST Evaluation</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p><a class="external-link" href="https://arxiv.org/abs/2405.02559" rel="nofollow" target="_blank">https://arxiv.org/abs/2405.02559</a></p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>“QUEST” -Quality of Information, Understanding and Reasoning, Expression Style and Persona, Safety and Harm, and Trust and Confidence, talks about providing explanation for LLM with the following metrics such as Uncertainty, Relevance, coherence, Language Tone, Sentiment Analysis etc.</p></td>
<td class="confluenceTd" style="border: 1px solid #ddd; padding: 8px; text-align: left;"><p>Implemented these Evaluation metrics to support LLM Explanation</p></td>
</tr>
</tbody>
</table></div>
<style>html .dt1004759711 table {min-width:532px;}</style>
</div>

</div>
</div>

<hr/>
<h2 id="UnderstandingExplainabilityinAIEUAIActNISTISO42001"><span data-colorid="d01m6t726y">Understanding Explainability in AI: EU AI Act, NIST, ISO42001</span><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<h3 id="NISTNationalInstituteofStandardsandTechnologyAIRMF"><span data-colorid="og38apk4dg">NIST (National Institute of Standards and Technology) - AI RMF</span><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<div class="expand-container conf-macro output-block" data-hasbody="true" data-macro-id="fe16cd86-ef36-4ccf-ac01-4386f46583d6" data-macro-name="expand" id="expander-1724572535">
<div class="expand-control" id="expander-control-1724572535" onclick="expandContent('expander-content-1724572535', 'expander-control-1724572535')">
<span class="expand-control-text">NIST Guidelines - Explainability</span>
</div>
<div class="expand-content expand-hidden" id="expander-content-1724572535">
<p>NIST’s Artificial Intelligence Risk Management Framework (AI RMF) has proposed four key principles for explainable AI (XAI) to promote trust and transparency in AI systems:</p>
<ul>
<li><p><u>Explanation:</u> AI systems should provide accompanying evidence or reasons for all outputs. This means that users should be able to understand why the system made a particular decision.</p></li>
</ul>
<p>e.g.: A self-driving car system should be able to explain why it decided to brake suddenly, such as by highlighting the detected object, the distance to it, and the speed of the car.</p>
<ul>
<li><p><u>Meaningful:</u> Explanations should be understandable to individual users, regardless of their technical expertise. This requires tailoring explanations to the specific needs and knowledge level of the user.</p></li>
</ul>
<p>e.g.: A loan approval system should explain the decision in terms that a non-technical user can understand, such as using plain language instead of technical jargon.</p>
<ul>
<li><p><u>Explanation Accuracy:</u> Explanations should accurately reflect the system's process for generating the output. This ensures that users can trust the explanations provided by the system.</p></li>
</ul>
<p>e.g.: An image classification system should not provide an explanation that is based on a misunderstanding of the image content, such as mistaking a cat for a dog.</p>
<ul>
<li><p><u>Knowledge Limits:</u> AI systems should only operate under conditions for which they were designed and when they reach sufficient confidence in their output. This helps to prevent the system from making decisions that are beyond its capabilities or that are based on insufficient information.</p></li>
</ul>
<p>e.g.: A weather forecasting system should indicate when it is uncertain about its predictions, such as by providing a confidence level or range of possible outcomes.</p>

</div>
</div>
<h3 id="EuropeanUnionArtificialIntelligenceActEUAIAct"><span data-colorid="waje4ptpit">European Union Artificial Intelligence Act (EU AI Act)</span><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<div class="expand-container conf-macro output-block" data-hasbody="true" data-macro-id="a44e7a55-e76e-4bb4-afc9-4ed9f04a6373" data-macro-name="expand" id="expander-436857594">
<div class="expand-control" id="expander-control-436857594" onclick="expandContent('expander-content-436857594', 'expander-control-436857594')">
<span class="expand-control-text">EU AI Act - Explainability</span>
</div>
<div class="expand-content expand-hidden" id="expander-content-436857594">
<ul>
<li><p>Explainability by Design: AI systems should be designed to be explainable from the outset, rather than as an afterthought. This means that developers should consider explainability throughout the development process.</p></li>
</ul>
<p>e.g.: Use LIME or SHAP for understanding AI model decisions. Choose interpretable models like decision trees.</p>
<ul>
<li><p>Human-Understandable Explanations: Explanations should be provided in a way that is understandable to humans, regardless of their technical expertise. This may involve using natural language, visualizations, or other methods.</p></li>
</ul>
<p>e.g.: An AI system that recommends products generates explanations like "We recommended this product because it aligns with your past purchase history and preferences."</p>
<ul>
<li><p>Contextual Explanations: Explanations should be provided in the context of the specific use case. For example, an explanation of a credit scoring algorithm should be tailored to the specific needs of the lender and the borrower.</p></li>
</ul>
<p>e.g.: A financial AI system explains a loan rejection by providing examples of similar cases where loans were denied due to similar reasons.</p>
<ul>
<li><p>Transparency of Decision-Making: AI systems should be transparent in their decision-making processes. This means that users should be able to understand how the system reached a particular decision.</p></li>
</ul>
<p>e.g.: An AI system that predicts customer churn provides information about the most important factors that contribute to the prediction, such as customer tenure, recent purchase frequency, and customer satisfaction ratings.</p>
<ul>
<li><p>Accountability: Developers and operators of AI systems should be accountable for the decisions that their systems make. This includes being able to explain the reasoning behind those decisions.</p></li>
</ul>
<p>e.g.: Developers are required to provide documentation and training materials to ensure that operators understand how the AI system works and can address potential issues. Organizations establish ethical review boards to assess the potential risks and benefits of AI systems and ensure they are used responsibly.</p>

</div>
</div>
<h3 id="ISO42001InternationalStandardforAIGovernance"><span data-colorid="y6hgeprk2p">ISO 42001 - International Standard for AI Governance</span><a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h3>
<div class="expand-container conf-macro output-block" data-hasbody="true" data-macro-id="2fe1ace6-d66e-4db9-8330-c747822bc429" data-macro-name="expand" id="expander-991865032">
<div class="expand-control" id="expander-control-991865032" onclick="expandContent('expander-content-991865032', 'expander-control-991865032')">
<span class="expand-control-text">Promotes responsible and ethical AI development and deployment. </span>
</div>
<div class="expand-content expand-hidden" id="expander-content-991865032">
<p>Being ISO 42001 certified organization, Infosys Responsible AI office adheres to applicable guidelines for Explainability implementation. Following is the snapshot of ISO42001 guidelines for Responsible AI implementation.</p>
<p> <u>Ensuring Trustworthiness</u></p>
<ul>
<li><p>Ethical AI: Adhering to ethical principles and values throughout the AI lifecycle.</p></li>
<li><p>Fairness: Mitigating biases and ensuring AI systems treat individuals fairly.</p></li>
<li><p>Transparency: Providing clear explanations for AI decision-making processes.</p></li>
<li><p>Accountability: Taking responsibility for the development, deployment, and use of AI systems.</p></li>
<li><p>Safety: Prioritizing safety and minimizing risks associated with AI applications.</p></li>
</ul>
<p><u>Promoting Human-Centric AI</u></p>
<ul>
<li><p>User Experience: Considering the needs and experiences of users when designing and deploying AI systems.</p></li>
<li><p>Inclusivity: Ensuring AI systems are accessible and inclusive for diverse populations.</p></li>
</ul>
<p><u>Managing AI Risks</u></p>
<ul>
<li><p>Risk Assessment: Identifying and assessing potential risks associated with AI Systems.</p></li>
<li><p>Mitigation Strategies: Implementing measures to mitigate identified risks.</p></li>
<li><p>Continuous Monitoring: Regular monitoring and evaluating AI systems for emerging risks.</p></li>
</ul>
<p><u>Demonstrating Commitment to Responsible AI</u></p>
<ul>
<li><p>Credibility: Establishing credibility and trust with stakeholders.</p></li>
<li><p>Competitive Advantage: Gaining a competitive advantage by demonstrating a commitment to responsible AI practices.</p></li>
<li><p>Regulatory Compliance: Meeting regulatory requirements related to AI governance.</p></li>
</ul>
<p><u>Driving Innovation</u></p>
<ul>
<li><p>Ethical Innovation: Fostering innovation while adhering to ethical principles.</p></li>
<li><p>Responsible Development: Developing AI systems that contribute positively to society.</p></li>
</ul>
</div>
</div>
<h2 id="Glossaryoftermsused"><span data-colorid="wixhf667pv">Glossary of terms used</span> <a class="anchor-copy-link anchor-tooltip" target="_blank"></a></h2>
<div class="expand-container conf-macro output-block" data-hasbody="true" data-macro-id="1a23f03f-2b1e-4cab-bf84-47223b8cf45d" data-macro-name="expand" id="expander-1618883168">
<div class="expand-control" id="expander-control-1618883168" onclick="expandContent('expander-content-1618883168', 'expander-control-1618883168')">
<span class="expand-control-text">Description of terms used in explainability</span>
</div>
<div class="expand-content expand-hidden" id="expander-content-1618883168">
<ol start="1">
<li><p><strong>Feature Ranking:</strong> To identify the most important features that contribute significantly to the model's predictions.</p></li>
<li><p><strong>Model Understanding:</strong> To understand how different features interact and influence the model's outcomes.</p></li>
<li><p><strong>Feature Selection:</strong> To identify redundant or irrelevant features that can be removed without affecting performance.</p></li>
<li><p><strong>Bias Detection:</strong> To detect potential biases in the model by identifying features that disproportionately influence predictions.</p></li>
<li><p><strong>Feature Importance:</strong> To identify the most important features that contribute significantly to the model's predictions.</p></li>
<li><p><strong>Debugging:</strong> To identify and correct errors in the model's predictions.</p></li>
<li><p><strong>Local Interpretability:</strong> To provide explanations tailored to individual predictions, rather than global explanations that apply to the entire model.</p></li>
<li><p><strong>Model Agnosticism:</strong> Can be applied to any machine learning model, regardless of its complexity or architecture.</p></li>
<li><p><strong>Simplicity:</strong> Generates simple, human-understandable rules that can be easily interpreted.</p></li>
<li><p><strong>Causality:</strong> Can help to identify causal relationships between input features and the model's predictions.</p></li>
<li><p><strong>Fairness:</strong> Can be used to detect and mitigate biases in the model by identifying features that disproportionately influence predictions.</p></li>
<li><p><strong>Visual Representation:</strong> Decision trees can be visualized as tree-like structures, making it easy to understand the decision-making process.</p></li>
<li><p><strong>Rule Extraction:</strong> Rules can be extracted from decision trees, providing human-readable explanations of the model's predictions.</p></li>
<li><p><strong>Knowledge Extraction:</strong> Can be used to extract knowledge from the model that can be applied to other tasks.</p></li>
<li><p><strong>Approximation:</strong> Surrogate models can be used to approximate the behavior of a complex model in a specific region of the input space.</p></li>
<li><p><strong>Step-by-Step Breakdown:</strong> These frameworks break down complex problems into smaller, more manageable steps, making it easier to follow the LLM's reasoning.</p></li>
<li><p><strong>Coherent Narratives:</strong> By generating interconnected thoughts or a graph representation, these techniques create a more coherent and understandable explanation.</p></li>
<li><p><strong>Accuracy and Reliability:</strong> CoV helps to ensure the accuracy and reliability of the LLM's outputs by verifying them against external sources.</p></li>
<li><p><strong>Transparency:</strong> These techniques provide transparency into the LLM's decision-making process, allowing users to understand how the model arrived at its conclusions.</p></li>
</ol>
</div>
</div>
</div>
<!-- ATTACHMENTS -->

<script>
            document.addEventListener("DOMContentLoaded", () => {
                const wrapper = document.getElementById("attachments-wrapper");
                const button = document.getElementById("toggle-attachments-view-button");
                document.querySelectorAll(".file-full").forEach(el => {
                    el.addEventListener("mouseover", moveTooltip);
                });

                button.addEventListener("click", () => {
                    wrapper.classList.toggle("attachments-wrapper-gallery");
                    wrapper.classList.toggle("attachments-wrapper-list");
                });
            });

            function moveTooltip(e) {
                if (e.target.classList.contains("file-wrapper")) {
                    let docWidth = document.body.clientWidth;
                    let docHeight = document.body.clientHeight;
                    let rect = e.target.getBoundingClientRect();
                    let fileTooltip = e.target.parentElement.querySelector(".file-tooltip")
                    if (fileTooltip) {
                        if (rect.left <= docWidth / 2) {
                            fileTooltip.classList.add("left");
                            fileTooltip.classList.remove("right");
                        } else {
                            fileTooltip.classList.remove("left");
                            fileTooltip.classList.add("right");
                        }
                        if (rect.top <= docHeight / 2) {
                            fileTooltip.classList.add("top");
                            fileTooltip.classList.remove("bottom");
                        } else {
                            fileTooltip.classList.remove("top");
                            fileTooltip.classList.add("bottom");
                        }
                    }
                }
            }

        </script>
<script>
                hideGroup('attachments');
            </script>
<div id="footer-comments-outlet">
<div>
<div class="page-comment-wrapper" data-testid="page-comment-wrapper">
<div class="cc-q82yp6">
<div class="_1e0c1txw _i0dl1osq _otyru2gc">

<div class="_1e0c11p5 _yv0ehpgh _727q19bv _bfhk1j28 _1bsbdgin _18u0u2gc">


</div>
</div>

<div class="_1e0c1txw _1n261g80" data-testid="comment-container">
<div class="_1e0c1txw _i0dl1osq _otyru2gc">


</div>
</div>
</div>
</div>
</div>
</div>

</div>
</div></body><br/><br/></html>