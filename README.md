# **Infosys-Responsible-AI-Toolkit**
The Infosys Responsible AI toolkit provides a set of APIs to integrate safety, privacy, security, explainability, fairness, and hallucination detection into AI solutions, ensuring trustworthiness and transparency. 

To install any Infosys Responsible AI Toolkit module, open the respective repository and follow the instructions in the README file.

| No. | Module name | Functionalities | Repository names |
|---|---|---|---|
| 1 | ModerationLayer APIs  | Moderates Input and Output of LLMs | responsible-ai-moderationlayer,responsible-ai-moderationModel |
| 2 | Explainability APIs  | Get Explainability to LLM responses, Global and local explainability for Regression, Classification and Timeseries Models | responsible-ai-llm-explain, 
responsible-ai-explainability,
responsible-ai-moderationLayer |
| 3 | Fairness & Bias API  | Check Fairness and detect Biases associated with LLM prompts and responses and also for traditional ML models | responsible-ai-fairness |
| 4 | Hallucination API  | Detect and quantify Hallucination in LLM responses under RAG scenarios | responsible-ai-hallucination |
| 5 | Privacy API  | Detect and anonymize or encrypt or hilight PII information in prompts for LLMs or in its responses | responsible-ai-privacy |
| 6 | Security API  | Different types of security attacks and defenses on tabular and image data, prompt injection and jailbreak checks | responsible-ai-security-API,
responsible-ai-llm-security |
| 7 | Safety API  | Detects and anonymize toxic and profane text associated with LLMs | responsible-ai-safety |


These API-based guardrails are being hosted on platforms like Azure OpenAI.

If you have more questions or need further insights please feel free to connect with us  Infosysraitoolkit@infosys.com

